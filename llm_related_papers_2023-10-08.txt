1. Title: MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical  Reasoning
   Link: https://arxiv.org/abs/2310.03731
   Classification Answer: yes

   Abstract: The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at this https URL.

  Translated Abstract: 最近发布的GPT-4代码解释器表明，在解决具有挑战性的数学问题方面非常熟练，主要归因于其使用自然语言无缝推理，生成代码，执行代码并根据执行输出继续推理的能力。在本文中，我们提出了一种微调开源语言模型的方法，使他们能够使用代码来建模和得出数学方程，从而增强其数学推理能力。我们提出了一种使用数学问题及其基于代码的解决方案（称为MathCodeInstruct）生成新颖和高质量数据集的方法。每个解决方案都交织了自然语言，代码和执行结果。我们还引入了定制的监督微调和推理方法。这种方法产生了MathCoder模型，该模型是一个模型家族，能够生成基于代码的解决方案，以解决挑战性的数学问题。令人印象深刻的是，MathCoder模型在数学（45.2％）和GSM8K（83.9％）数据集中达到开源LLM的最先进得分，这显然优于其他开源替代方案。值得注意的是，MathCoder模型不仅超过了GSM8K和MATH上的Chatgpt-3.5和Palm-2，而且在竞争级数学数据集上的表现优于GPT-4。数据集和模型将在此HTTPS URL上发布。


2. Title: Agent Instructs Large Language Models to be General Zero-Shot Reasoners
   Link: https://arxiv.org/abs/2310.03710
   Classification Answer: yes

   Abstract: We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.

  Translated Abstract: 我们介绍了一种在一般语言理解任务上提高大语言模型的零摄像推理能力的方法。具体来说，我们建立一个自主代理来指导大语模型的推理过程。我们表明，这种方法进一步将大语言模型的零射击推理能力释放到了更多任务。我们研究方法在跨越一系列的数据集上的性能，这些数据集跨越了生成，分类和推理。我们表明，我们的方法将概括为大多数任务，并在我们评估的29个数据集中的20个数据集中获得最先进的零拍摄性能。例如，我们的方法将最先进的大语言模型的性能提高了很大的利润，包括Vicuna-13b（13.3％），Llama-2-70B-Chat（23.2％）和GPT 3.5 Turbo（17.0％）。与零射的思想链相比，我们的推理改善是惊人的，平均增加了10.5％。通过我们的方法，Llama-2-70B-Chat的表现优于零射门GPT-3.5涡轮增压器10.2％。


3. Title: Fine-tuning Aligned Language Models Compromises Safety, Even When Users  Do Not Intend To!
   Link: https://arxiv.org/abs/2310.03693
   Classification Answer: yes

   Abstract: Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.

  Translated Abstract: 优化大型语言模型（LLMS）用于下游用例通常涉及通过进一步的微调来定制预训练的LLMS。Meta在自定义数据集上进行微调GPT-3.5涡轮增压的Llama模型和OpenAI的API也鼓励了这种做法。但是，与这种自定义微调相关的安全费用是多少？我们注意到，尽管现有的安全一致性基础架构可以在推理时限制LLM的有害行为，但在将微调特权扩展到最终用户时，它们并不涵盖安全风险。我们的红色团队研究发现，通过仅使用几个对抗设计的训练示例进行微调，可以通过微调来损害LLM的安全一致性。例如，我们通过Openai的API仅在10个此类示例上微调了10个此类示例，从而越狱GPT-3.5 Turbo的安全护栏，使该模型几乎可以响应任何有害说明。令人不安的是，我们的研究还表明，即使没有恶意意图，简单地对良性和常用数据集进行微调也可以无意中降低LLM的安全对准，尽管程度较小。这些发现表明，微调对齐的LLMS引入了新的安全风险，即当前的安全基础设施无法解决 - 即使模型的初始安全对齐是无可挑剔的，也不一定要在自定义微调后维护。我们概述和批判性地分析了潜在的缓解，并倡导进一步的研究工作，以加强安全协议，以定制对齐的LLM。


4. Title: MapperGPT: Large Language Models for Linking and Mapping Entities
   Link: https://arxiv.org/abs/2310.03666
   Classification Answer: yes

   Abstract: Aligning terminological resources, including ontologies, controlled vocabularies, taxonomies, and value sets is a critical part of data integration in many domains such as healthcare, chemistry, and biomedical research. Entity mapping is the process of determining correspondences between entities across these resources, such as gene identifiers, disease concepts, or chemical entity identifiers. Many tools have been developed to compute such mappings based on common structural features and lexical information such as labels and synonyms. Lexical approaches in particular often provide very high recall, but low precision, due to lexical ambiguity. As a consequence of this, mapping efforts often resort to a labor intensive manual mapping refinement through a human curator.
Large Language Models (LLMs), such as the ones employed by ChatGPT, have generalizable abilities to perform a wide range of tasks, including question-answering and information extraction. Here we present MapperGPT, an approach that uses LLMs to review and refine mapping relationships as a post-processing step, in concert with existing high-recall methods that are based on lexical and structural heuristics.
We evaluated MapperGPT on a series of alignment tasks from different domains, including anatomy, developmental biology, and renal diseases. We devised a collection of tasks that are designed to be particularly challenging for lexical methods. We show that when used in combination with high-recall methods, MapperGPT can provide a substantial improvement in accuracy, beating state-of-the-art (SOTA) methods such as LogMap.

  Translated Abstract: 将术语资源（包括本体论，受控词汇，分类法和价值集）对齐是许多领域中数据集成的关键部分，例如医疗保健，化学和生物医学研究。实体映射是确定这些资源之间实体之间对应关系的过程，例如基因标识符，疾病概念或化学实体标识符。已经开发了许多工具来根据常见的结构特征和词汇信息（例如标签和同义词）来计算此类映射。特别是词汇方法通常会由于词汇歧义而提供很高的回忆，但精度很低。因此，映射工作经常通过人类策展人求助于劳动密集型手动映射精炼。
大型语言模型（LLM），例如ChatGpt使用的模型，具有可概括的能力，可以执行各种任务，包括提问和信息提取。在这里，我们提出了Mappergpt，该方法使用LLMS来审查和完善映射关系作为后处理步骤，并与现有基于词汇和结构启发式方法的现有高回报方法。
我们评估了来自不同领域的一系列对齐任务的Mappergpt，包括解剖学，发育生物学和肾脏疾病。我们设计了一系列任务，这些任务旨在对词汇方法特别具有挑战性。我们表明，当与高回报方法结合使用时，Mappergpt可以提供准确性的实质性提高，击败最新方法（SOTA）方法（例如Logmap）。


5. Title: Redefining Digital Health Interfaces with Large Language Models
   Link: https://arxiv.org/abs/2310.03560
   Classification Answer: yes

   Abstract: Digital health tools have the potential to significantly improve the delivery of healthcare services. However, their use remains comparatively limited due, in part, to challenges surrounding usability and trust. Recently, Large Language Models (LLMs) have emerged as general-purpose models with the ability to process complex information and produce human-quality text, presenting a wealth of potential applications in healthcare. Directly applying LLMs in clinical settings is not straightforward, with LLMs susceptible to providing inconsistent or nonsensical answers. We demonstrate how LLMs can utilize external tools to provide a novel interface between clinicians and digital technologies. This enhances the utility and practical impact of digital healthcare tools and AI models while addressing current issues with using LLM in clinical settings such as hallucinations. We illustrate our approach with examples from cardiovascular disease and diabetes risk prediction, highlighting the benefit compared to traditional interfaces for digital tools.

  Translated Abstract: 数字保健工具有可能显着改善医疗服务的交付。但是，它们的使用仍然相对有限，部分原因是围绕可用性和信任的挑战。最近，大型语言模型（LLM）已成为通用模型，具有处理复杂信息并产生人类质量文本的能力，并在医疗保健中提供了大量潜在应用。在临床环境中直接应用LLM并不直接，LLMS很容易提供不一致或荒谬的答案。我们展示了LLM如何利用外部工具在临床医生和数字技术之间提供新颖的接口。这增强了数字医疗工具和AI模型的实用性和实际影响，同时解决了在临床环境（例如幻觉）中使用LLM的当前问题。我们用心血管疾病和糖尿病风险预测的例子说明了我们的方法，与传统的数字工具接口相比，该方法强调了收益。


6. Title: Towards Robust and Generalizable Training: An Empirical Study of Noisy  Slot Filling for Input Perturbations
   Link: https://arxiv.org/abs/2310.03518
   Classification Answer: yes

   Abstract: In real dialogue scenarios, as there are unknown input noises in the utterances, existing supervised slot filling models often perform poorly in practical applications. Even though there are some studies on noise-robust models, these works are only evaluated on rule-based synthetic datasets, which is limiting, making it difficult to promote the research of noise-robust methods. In this paper, we introduce a noise robustness evaluation dataset named Noise-SF for slot filling task. The proposed dataset contains five types of human-annotated noise, and all those noises are exactly existed in real extensive robust-training methods of slot filling into the proposed framework. By conducting exhaustive empirical evaluation experiments on Noise-SF, we find that baseline models have poor performance in robustness evaluation, and the proposed framework can effectively improve the robustness of models. Based on the empirical experimental results, we make some forward-looking suggestions to fuel the research in this direction. Our dataset Noise-SF will be released at this https URL.

  Translated Abstract: 在实际对话方案中，由于话语中存在未知的输入噪声，因此现有的监督老虎机填充模型通常在实际应用中表现不佳。尽管有一些关于噪声模型的研究，但这些作品仅在基于规则的合成数据集上进行评估，这是限制的，这使得很难促进对噪声方法的研究。在本文中，我们引入了一个名为noise-sf的噪声稳健性评估数据集用于插槽填充任务。所提出的数据集包含五种类型的人类通知噪声，所有这些声音都完全存在于实际的广泛的鲁棒训练方法中，将插槽填充到所提出的框架中。通过对Noise-SF进行详尽的经验评估实验，我们发现基线模型在鲁棒性评估中的性能较差，并且提出的框架可以有效地改善模型的鲁棒性。根据经验实验结果，我们提出了一些前瞻性的建议，以朝着这一方向推动研究。我们的数据集噪声-SF将在此HTTPS URL上发布。


7. Title: Tik-to-Tok: Translating Language Models One Token at a Time: An  Embedding Initialization Strategy for Efficient Language Adaptation
   Link: https://arxiv.org/abs/2310.03477
   Classification Answer: yes

   Abstract: Training monolingual language models for low and mid-resource languages is made challenging by limited and often inadequate pretraining data. In this study, we propose a novel model conversion strategy to address this issue, adapting high-resources monolingual language models to a new target language. By generalizing over a word translation dictionary encompassing both the source and target languages, we map tokens from the target tokenizer to semantically similar tokens from the source language tokenizer. This one-to-many token mapping improves tremendously the initialization of the embedding table for the target language. We conduct experiments to convert high-resource models to mid- and low-resource languages, namely Dutch and Frisian. These converted models achieve a new state-of-the-art performance on these languages across all sorts of downstream tasks. By reducing significantly the amount of data and time required for training state-of-the-art models, our novel model conversion strategy has the potential to benefit many languages worldwide.

  Translated Abstract: 培训对低资源语言的单语语言模型被有限且通常不足的预处理数据变得具有挑战性。在这项研究中，我们提出了一种新型的模型转换策略来解决这个问题，将高资源的单语言模型调整为一种新的目标语言。通过概括一个涵盖源和目标语言的单词翻译字典，我们将令牌从目标令牌映射到来自源语言令牌的语义上相似令牌。这个一对多的令牌映射可极大地改善目标语言的嵌入式表的初始化。我们进行实验，以将高资源模型转换为荷兰语和弗里斯兰语中的低资源语言。这些转换后的模型在各种下游任务上在这些语言上实现了新的最新性能。通过大大减少培训最先进模型所需的数据和时间，我们的新型模型转换策略有可能使全球许多语言受益。


8. Title: Controllable Multi-document Summarization: Coverage & Coherence  Intuitive Policy with Large Language Model Based Rewards
   Link: https://arxiv.org/abs/2310.03473
   Classification Answer: yes

   Abstract: Memory-efficient large language models are good at refining text input for better readability. However, controllability is a matter of concern when it comes to text generation tasks with long inputs, such as multi-document summarization. In this work, we investigate for a generic controllable approach for multi-document summarization that leverages the capabilities of LLMs to refine the text. In particular, we train a controllable content extraction scheme to extract the text that will be refined by an LLM. The scheme is designed with a novel coverage and coherence intuitive policy, which is duly rewarded by a passively trained LLM. Our approach yields competitive results in the evaluation using ROUGE metrics and outperforms potential baselines in coherence, as per human evaluation.

  Translated Abstract: 记忆有效的大语言模型擅长精炼文本输入，以提高可读性。但是，当涉及长期输入的文本生成任务时，可控性是一个问题，例如多文件摘要。在这项工作中，我们研究了一种通用可控方法，用于多文件摘要，该方法利用了LLM的能力来完善文本。特别是，我们训练一个可控的内容提取方案，以提取将由LLM完善的文本。该计划的设计采用了新颖的覆盖范围和连贯的直观政策，该政策得到了被动训练的LLM的适当奖励。根据人类评估，我们的方法在使用胭脂指标的评估中产生了竞争性结果，并且在连贯性方面优于潜在的基准。


9. Title: Procedural Text Mining with Large Language Models
   Link: https://arxiv.org/abs/2310.03376
   Classification Answer: yes

   Abstract: Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction.

  Translated Abstract: 自然语言处理领域的最新进展，尤其是在大量知识上鉴定的大规模语言模型的发展，正在在知识工程领域创造新的机会。在本文中，我们研究了大语言模型（LLMS）在零击和内在学习设置中的使用情况，以以逐步提问的方式从非结构化的PDF文本中提取程序的问题。特别是，我们利用当前最新的GPT-4（生成预训练的变压器4）模型，并伴随着两种涉及程序和步骤的定义的内在学习的变体，该本体涉及一个本体。少数学习的样本。这些发现既突出了这种方法的承诺，也凸显了内在学习自定义的价值。这些修改有可能显着解决获得足够的培训数据的挑战，这是一个基于深度学习的自然语言处理技术以进行过程提取的障碍。


10. Title: Evaluating Hallucinations in Chinese Large Language Models
   Link: https://arxiv.org/abs/2310.03368
   Classification Answer: yes

   Abstract: In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated. We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50%. This indicates that HalluQA is highly challenging. We analyze the primary types of hallucinations in different types of models and their causes. Additionally, we discuss which types of hallucinations should be prioritized for different types of models.

  Translated Abstract: 在本文中，我们建立了一个名为Halluqa（中国幻觉提问）的基准，以测量中国大语言模型的幻觉现象。Halluqa包含450个精心设计的对抗性问题，涵盖了多个领域，并考虑了中国历史文化，习俗和社会现象。在Halluqa的构建过程中，我们考虑了两种类型的幻觉：模仿性虚假和事实错误，我们根据GLM-130B和Chatgpt构建了对抗性样本。为了进行评估，我们使用GPT-4设计了一种自动评估方法，以判断模型输出是否是幻觉的。我们对24个大型语言模型进行了广泛的实验，包括Ernie-Bot，Baichuan2，Chatglm，Qwen，Sparkdesk等。在24个模型中，18个实现的非抗解率低于50％。这表明Halluqa极具挑战性。我们分析了不同类型的模型及其原因中幻觉的主要类型。此外，我们讨论了不同类型的模型应优先考虑哪种类型的幻觉。


11. Title: Reformulating Domain Adaptation of Large Language Models as  Adapt-Retrieve-Revise
   Link: https://arxiv.org/abs/2310.03328
   Classification Answer: yes

   Abstract: While large language models (LLMs) like GPT-4 have recently demonstrated astonishing zero-shot capabilities in general domain tasks, they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.
This paper introduces a simple and effective domain adaptation framework for GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process. The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain by continuing learning on in-domain data. When solving a task, we leverage the adapted LLM to generate a draft answer given a task query. Then, the draft answer will be used to \textbf{retrieve} supporting evidence candidates from an external in-domain knowledge base. Finally, the draft answer and retrieved evidence are concatenated into a whole prompt to let GPT-4 assess the evidence and \textbf{revise} the draft answer to generate the final answer.
Our proposal combines the advantages of the efficiency of adapting a smaller 7B model with the evidence-assessing capability of GPT-4 and effectively prevents GPT-4 from generating hallucinatory content. In the zero-shot setting of four Chinese legal tasks, our method improves accuracy by 33.3\% compared to the direct generation by GPT-4. When compared to two stronger retrieval-based baselines, our method outperforms them by 15.4\% and 23.9\%. Our code will be released

  Translated Abstract: 尽管像GPT-4这样的大型语言模型（LLM）最近在一般领域任务中表现出惊人的零射击功能，但它们经常在中国法律等特定领域中产生幻觉的内容，从而阻碍了他们在这些领域的应用。这通常是由于缺乏包含此类特定领域的培训数据，从而阻止GPT-4获得内域知识。一个紧迫的挑战是，继续培训这种规模的llms在内域数据上是不合理的。
本文通过将生成重新规定为\ textbf {Adapt-retrieve-Revise}过程，为GPT-4介绍了一个简单有效的域适应框架。最初的步骤是\ textbf {apapt}通过继续对内域数据学习，可负担的7B LLM对目标域。解决任务时，我们利用改编的LLM给定任务查询生成草稿答案。然后，答案草案将用于\ textbf {检索}从外部内域知识库中支持候选的证据。最后，将答案和检索证据草案置于整个提示中，让GPT-4评估证据，\ textbf {restise}答案草案生成了最终答案。
我们的建议结合了适应较小的7b模型与GPT-4的证据评估能力的效率的优势，并有效防止GPT-4产生幻觉含量。在四个中国法律任务的零拍摄设置中，与GPT-4的直接生成相比，我们的方法提高了33.3 \％。与两个基于检索的基线相比，我们的方法的表现优于15.4 \％和23.9 \％。我们的代码将发布


12. Title: Concise and Organized Perception Facilitates Large Language Models for  Deductive Reasoning
   Link: https://arxiv.org/abs/2310.03309
   Classification Answer: yes

   Abstract: Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists them in answering questions or drawing conclusions precisely and quickly. In light of this, we propose a novel reasoning approach named Concise and Organized Perception (COP). COP carefully analyzes the given statements to efficiently identify the most pertinent information while eliminating redundancy. It then prompts the LLMs in a more organized form that adapts to the model's inference process. By perceiving concise and organized proofs, the deductive reasoning abilities of LLMs can be better elicited, and the risk of acquiring errors caused by excessive reasoning stages is mitigated. Furthermore, our approach can be combined with the aforementioned ones to further boost their performance. Extensive experimental results on three popular deductive benchmarks (i.e., ProofWriter, PrOntoQA and PrOntoQA-OOD) show that COP significantly outperforms previous state-of-the-art methods.

  Translated Abstract: 利用大型语言模型（LLM）来解决演绎推理，引起了人们的关注。在复杂的演绎问题中取得令人满意的结果，其特征是大量前提（即事实或规则），这仍然是高度挑战。一种直观的解决方案是将原始任务分解为较小的子任务，然后将多个休闲推理步骤链在前方（例如，选择 - 推动）或向后（例如Lambada）方向上。但是，这些技术不可避免地需要大量的整体阶段，从而导致计算昂贵的操作以及更高的可能性进行误导性步骤。除了阶段的分解外，我们还从人类解决问题的另一个方面汲取灵感。人类倾向于提炼最相关的信息并系统地组织他们的思想（例如，创建思维图），这有助于他们回答问题或精确，快速得出结论。鉴于此，我们提出了一种新颖的推理方法，称为简洁和有组织的感知（COP）。COP仔细分析给定的语句，以有效地识别最相关的信息，同时消除冗余。然后，它以更有条理的形式提示LLM，以适应模型的推理过程。通过感知简洁和有组织的证据，可以更好地引起LLMS的演绎推理能力，并且可以减轻因过度推理阶段而造成的错误的风险。此外，我们的方法可以与上述方法相结合，以进一步提高其性能。对三个流行的演绎基准（即证明作者，Prontoqa和prontoqa-ood）的广泛实验结果表明，COP明显优于先前的最新方法。


13. Title: A New Dialogue Response Generation Agent for Large Language Models by  Asking Questions to Detect User's Intentions
   Link: https://arxiv.org/abs/2310.03293
   Classification Answer: yes

   Abstract: Large Language Models (LLMs), such as ChatGPT, have recently been applied to various NLP tasks due to its open-domain generation capabilities. However, there are two issues with applying LLMs to dialogue tasks. 1. During the dialogue process, users may have implicit intentions that might be overlooked by LLMs. Consequently, generated responses couldn't align with the user's intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively. In certain specific domains, their knowledge may be incomplete, and LLMs cannot update the latest knowledge in real-time. To tackle these issues, we propose a framework~\emph{using LLM to \textbf{E}nhance dialogue response generation by asking questions to \textbf{D}etect user's \textbf{I}mplicit in\textbf{T}entions} (\textbf{EDIT}). Firstly, EDIT generates open questions related to the dialogue context as the potential user's intention; Then, EDIT answers those questions by interacting with LLMs and searching in domain-specific knowledge bases respectively, and use LLMs to choose the proper answers to questions as extra knowledge; Finally, EDIT enhances response generation by explicitly integrating those extra knowledge. Besides, previous question generation works only focus on asking questions with answers in context. In order to ask open questions, we construct a Context-Open-Question (COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and Holl-E), EDIT outperformed other LLMs.

  Translated Abstract: 大型语言模型（LLM），例如ChatGpt，由于其开放域的生成功能，最近已应用于各种NLP任务。但是，将LLMS应用于对话任务有两个问题。1.在对话过程中，用户可能具有LLM可能忽略的隐式意图。因此，生成的响应不能与用户的意图保持一致。2. LLM不太可能全面涵盖所有字段。在某些特定领域，他们的知识可能不完整，而LLMS无法实时更新最新知识。为了解决这些问题，我们提出了一个框架〜\ emph {使用llm to \ textbf {e} nhance对话响应响应生成，通过向\ textbf {d}提出问题，以\ textbf {d} ext用户的\ textbf {i} mplicit（\ textbf {edit}）。首先，编辑生成与对话上下文有关的开放问题，作为潜在用户的意图；然后，编辑通过与LLM进行互动和在特定于领域的知识库中进行搜索来回答这些问题，并使用LLMS选择问题的适当答案作为额外的知识；最后，编辑通过明确整合这些额外知识来增强响应生成。此外，以前的问题生成只专注于在上下文中提出问题。为了提出开放问题，我们构建了一个上下文开放的问题（COQ）数据集。在两个面向任务的对话任务（Wikipedia和Holl-E的向导）上，编辑优于其他LLMS。


14. Title: A Formalism and Approach for Improving Robustness of Large Language  Models Using Risk-Adjusted Confidence Scores
   Link: https://arxiv.org/abs/2310.03283
   Classification Answer: yes

   Abstract: Large Language Models (LLMs), such as ChatGPT, have achieved impressive milestones in natural language processing (NLP). Despite their impressive performance, the models are known to pose important risks. As these models are deployed in real-world applications, a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed. In this paper, we define and formalize two distinct types of risk: decision risk and composite risk. We also propose a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings. Finally, we propose a risk-adjusted calibration method called DwD for helping LLMs minimize these risks in an overall NLI architecture. Detailed experiments, using four NLI benchmarks, three baselines and two LLMs, including ChatGPT, show both the practical utility of the evaluation framework, and the efficacy of DwD in reducing decision and composite risk. For instance, when using DwD, an underlying LLM is able to address an extra 20.1% of low-risk inference tasks (but which the LLM erroneously deems high-risk without risk adjustment) and skip a further 19.8% of high-risk tasks, which would have been answered incorrectly.

  Translated Abstract: 大型语言模型（LLM），例如ChatGpt，在自然语言处理（NLP）方面取得了令人印象深刻的里程碑。尽管表现令人印象深刻，但这些模型仍带来重要的风险。由于这些模型被部署在现实世界应用程序中，因此非常需要对这些模型对这些模型构成的不同风险的系统理解，例如自然语言推断（NLI）。在本文中，我们定义并形式化了两种不同类型的风险：决策风险和综合风险。我们还提出了一个以风险为中心的评估框架，以及四个新颖的指标，用于评估在内域和室外设置中这些风险的LLM。最后，我们提出了一种称为DWD风险调整的校准方法，用于帮助LLMS最大程度地减少NLI体系结构中的这些风险。详细的实验，使用四个NLI基准，三个基线和两个LLM，包括CHATGPT，既显示了评估框架的实际实用性，又显示DWD在降低决策和综合风险方面的功效。例如，当使用DWD时，基础LLM能够解决低风险推理任务的20.1％（但LLM错误地认为高风险而无需调整风险），并跳过了另外19.8％的高风险任务，即这将被错误地回答。


15. Title: FreshLLMs: Refreshing Large Language Models with Search Engine  Augmentation
   Link: https://arxiv.org/abs/2310.03214
   Classification Answer: yes

   Abstract: Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as this http URL. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at this http URL and commit to updating it at regular intervals.

  Translated Abstract: 大多数大型语言模型（LLM）经过一次训练，并且从未更新；因此，他们缺乏动态适应我们不断变化的世界的能力。在这项工作中，我们在回答测试当前世界知识的问题的背景下对LLM生成的文本的事实进行了详细研究。具体来说，我们介绍了FreshQA，这是一种新颖的动态质量质量检查基准，其中包含各种各样的问题和答案类型，包括需要快速变化的世界知识的问题以及需要揭穿的错误前提的问题。在两种模式评估程序下，我们基准了各种封闭和开源LLM的各种阵列，该程序使我们能够衡量正确性和幻觉。通过涉及超过50k判断的人类评估，我们阐明了这些模型的局限性，并证明了重大改进的空间：例如，所有模型（无论模型规模如何）在涉及快速变化的知识和虚假前提的问题上挣扎。在这些结果的推动下，我们提出了FreshPrompt，这是一种简单的几弹性提示方法，通过将从搜索引擎检索到的相关和最新信息合并到提示符中，从而大大提高了LLM在FreshQA上的性能。我们的实验表明，FreshPrompt的表现优于竞争性搜索引擎的提示方法，例如自助式（Press等，2022）以及此HTTP URL等商业系统。对FreshPrompt的进一步分析表明，检索的证据的数量及其顺序在影响LLM生成的答案的正确性方面都起着关键作用。此外，与鼓励更多的详细答案相比，指导LLM生成简洁和直接答案有助于减少幻觉。为了促进未来的工作，我们在此HTTP URL上发布FreshQA，并致力于定期更新它。


16. Title: On the Performance of Multimodal Language Models
   Link: https://arxiv.org/abs/2310.03211
   Classification Answer: yes

   Abstract: Instruction-tuned large language models (LLMs) have demonstrated promising zero-shot generalization capabilities across various downstream tasks. Recent research has introduced multimodal capabilities to LLMs by integrating independently pretrained vision encoders through model grafting. These multimodal variants undergo instruction tuning, similar to LLMs, enabling effective zero-shot generalization for multimodal tasks. This study conducts a comparative analysis of different multimodal instruction tuning approaches and evaluates their performance across a range of tasks, including complex reasoning, conversation, image captioning, multiple-choice questions (MCQs), and binary classification. Through rigorous benchmarking and ablation experiments, we reveal key insights for guiding architectural choices when incorporating multimodal capabilities into LLMs. However, current approaches have limitations; they do not sufficiently address the need for a diverse multimodal instruction dataset, which is crucial for enhancing task generalization. Additionally, they overlook issues related to truthfulness and factuality when generating responses. These findings illuminate current methodological constraints in adapting language models for image comprehension and provide valuable guidance for researchers and practitioners seeking to harness multimodal versions of LLMs.

  Translated Abstract: 指导调节的大型语言模型（LLMS）已证明了各种下游任务的有希望的零击功能。最近的研究通过通过模型移植整合了独立的视力编码来引入LLM的多模式功能。这些多模式变体经历了类似于LLM的指令调整，从而实现了多模式任务的有效零弹性概括。这项研究对不同的多模式教学调谐方法进行了比较分析，并评估了它们在各种任务中的性能，包括复杂的推理，对话，图像字幕，多项选择问题（MCQ）和二进制分类。通过严格的基准测试和消融实验，我们揭示了将多模式能力纳入LLM时指导体系结构选择的关键见解。但是，当前的方法有局限性。他们没有足够的问题满足对多种多模式指令数据集的需求，这对于增强任务概括至关重要。此外，他们在产生回应时忽略了与真实和事实有关的问题。这些发现阐明了当前的方法论限制，以适应图像理解的语言模型，并为寻求利用LLMS多模式版本的研究人员和从业人员提供宝贵的指导。


17. Title: Large Language Model Cascades with Mixture of Thoughts Representations  for Cost-efficient Reasoning
   Link: https://arxiv.org/abs/2310.03094
   Classification Answer: yes

   Abstract: Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40% of its cost.

  Translated Abstract: GPT-4等大型语言模型（LLM）在各种任务中表现出色，但是这种强大的表现通常伴随着使用付费API服务的高昂费用。在本文中，我们有动力研究建立一个LLM级联以节省LLM的成本，特别是用于执行推理（例如数学，因果关系）任务的成本。我们的级联管道遵循这样的直觉，即更简单的问题可以通过较弱但更实惠的LLM来解决，而只有具有挑战性的问题才需要更强大，更昂贵的LLM。为了意识到这一决策，我们将弱LLM的“答案一致性”视为问题难度的信号，并提出了几种答案采样和一致性检查的方法，包括一个利用了两种思想表示形式的混合物（即链条， - 思想和经营计划）。通过在六个推理基准数据集上进行实验，GPT-3.5-Turbo和GPT-4分别是较弱，更强大的LLM，我们证明了我们提出的LLM Cascades可以实现与使用更强的LLM相当的性能，但仅需要40％它的成本。


18. Title: Discovering Knowledge-Critical Subnetworks in Pretrained Language Models
   Link: https://arxiv.org/abs/2310.03084
   Classification Answer: yes

   Abstract: Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized relational knowledge) but struggles to express the removed knowledge, and suffers performance drops on examples needing this removed knowledge on downstream tasks after finetuning.

  Translated Abstract: 审慎的语言模型（LMS）在其参数中编码知识的隐式表示。但是，将这些表示形式定位并彼此分解仍然是一个空旷的问题。在这项工作中，我们调查了验证的语言模型是否包含各种关键知识的子网：负责编码该模型已经记住的特定知识的特定稀疏计算子图。我们提出了一个多目标可区分的重量掩蔽方案，以发现这些子网，并表明我们可以使用它们来精确地删除模型中的特定知识，同时最大程度地减少对原始语言模型行为的不利影响。我们在多个GPT2变体上演示了我们的方法，揭示了完全负责特定关系的关系知识集合的高度稀疏子网（98％+）。当删除这些子网时，其余网络将保持其大部分初始能力（建模语言和其他记忆的关系知识），但努力表达删除的知识，并且在填补后需要删除这些知识的示例中，绩效下降了。


19. Title: How FaR Are Large Language Models From Agents with Theory-of-Mind?
   Link: https://arxiv.org/abs/2310.03051
   Classification Answer: yes

   Abstract: "Thinking is for Doing." Humans can infer other people's mental states from observations--an ability called Theory-of-Mind (ToM)--and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4's performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning.

  Translated Abstract: “思考是为了做。”人类可以从观察中推断出他人的精神状态 - 一种称为理论（汤姆）的能力 - 随后务实地采取这些推论。现有的问题回答基准（例如Tomi）提出模型问题，以推断故事中人物信念的推断，但不要测试模型是否可以使用这些推论来指导其行为。我们为大语模型（LLMS）提出了一个新的评估范式：进行（T4D）的思考，该范式要求模型将有关他人心理状态的推论与社交场景中的行动联系起来。T4D的实验表明，诸如GPT-4和Palm 2之类的LLM似乎在跟踪角色对故事的信念方面表现出色，但它们很难将这种能力转化为战略行动。我们的分析揭示了LLMS的核心挑战在于确定有关心理状态的隐含推断而不明确地被征求Tomi，这导致选择了T4D中的正确行动。为了弥合这一差距，我们引入了一个零射击的提示框架，预见并反思（FAR），该框架提供了一种推理结构，鼓励LLMS预测未来的挑战和有关潜在行动的理由。T4D的GPT-4的表现将GPT-4的性能从50％提高到71％，超过了其他提示方法，例如，诸如思想链和自言自语。此外，遥远地概括了各种分布的故事结构和场景，这些结构和场景也需要汤姆推断才能选择一个动作，始终优于其他方法，包括少数镜头的内在学习。


20. Title: How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English  ChatGPT Responses
   Link: https://arxiv.org/abs/2310.03031
   Classification Answer: yes

   Abstract: With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to thoroughly check the system's responses for biases as well as for syntactic and grammatical mistakes.

  Translated Abstract: 随着CHATGPT的引入，OpenAI制作的大型语言模型（LLM）可供IT专业知识有限的用户访问。但是，没有自然语言处理背景（NLP）背景的用户可能缺乏对LLM的正确了解。因此，对它们固有的局限性的认识将使系统的输出在面值中。在本文中，我们系统地分析了提示和生成的响应，以确定可能的问题问题，并特别关注性别偏见，用户在处理系统的输出时需要注意这些问题。如果提示女性，男性或中立的角度回答，我们将探索Chatgpt如何用英语和德语做出反应。在深入的调查中，我们检查了选定的提示并分析如果以相同的方式提示该系统的响应在多大程度上有所不同。在此基础上，我们表明Chatgpt确实对于帮助非IT用户的日常工作起草文本很有用。但是，彻底检查系统的偏见以及句法和语法错误绝对至关重要。


21. Title: Retrieval meets Long Context Large Language Models
   Link: https://arxiv.org/abs/2310.03025
   Classification Answer: yes

   Abstract: Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented LLaMA2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long context tasks including question answering and query-based summarization. It also outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.

  Translated Abstract: 扩展大型语言模型（LLM）的上下文窗口最近越来越受欢迎，而增加了通过检索的LLM的解决方案已有多年了。自然问题是：i）检索 - 授权与长上下文窗口，哪一个更适合下游任务？ii）可以将两种方法合并以获得两全其美吗？在这项工作中，我们通过使用两种最先进的LLMS，即专有43B GPT和Llama2-70B研究两种解决方案来回答这些问题。也许令人惊讶的是，我们发现使用“生成简单检索”的LLM具有4K上下文窗口，可以通过长上下文任务上的位置插值来实现与16K上下文窗口相当的性能，同时进行较少的计算。更重要的是，我们证明，无论其扩展上下文窗口大小如何，检索都可以显着提高LLM的性能。我们的最佳模型，带有32K上下文窗口的检索型Llama2-70B，在七个长上下文任务上的平均得分（包括问题答案和基于查询的摘要）上的平均得分优于GPT-3.5-Turbo-16k和Davinci003。它还优于其非逆转LLAMA2-70B-32K基线的余量，同时代价快得多。我们的研究提供了有关从业人员的LLM的检索仪选择与长篇小说扩展的一般见解。


22. Title: From Words to Watts: Benchmarking the Energy Costs of Large Language  Model Inference
   Link: https://arxiv.org/abs/2310.03003
   Classification Answer: yes

   Abstract: Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs -- despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies.
In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of the inference performance and inference energy costs of different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta AI on two generations of popular GPUs (NVIDIA V100 \& A100) and two datasets (Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in research and practice. We present the results of multi-node, multi-GPU inference using model sharding across up to 32 GPUs. To our knowledge, our work is the one of the first to study LLM inference performance from the perspective of computational and energy resources at this scale.

  Translated Abstract: 大型语言模型（LLM）由于其新的生成能力而爆炸了，远远超出了先前的最新能力。这些技术越来越多地在法律，金融和医学等各个领域中杠杆化。但是，这些模型面临重大的计算挑战，尤其是推理所需的计算和能源成本。推理能源成本已经比培训LLM的能源成本更少受到关注 - 尽管这些大型模型经常被要求进行现实推断（例如Chatgpt）。由于这些最先进的LLM会看到在各个领域的使用和部署的增加，因此对其资源利用率的更好理解对于节省成本，扩展性能，有效的硬件使用量和最佳推理策略至关重要。
在本文中，我们描述了用于研究使用LLMS推断的计算和能量利用的实验。我们基准并对Meta AI在两代流行的GPU上开发的不同大小的Llama的推理性能和推理能源成本进行了初步分析（NVIDIA V100 \＆A100））和两个数据集（羊驼和GSM8K），以反映研究和实践中LLM的多种任务/基准。我们介绍了使用模型碎片多达32 GPU的多节点，多GPU推理的结果。据我们所知，我们的工作是从这个规模的计算和能源的角度研究LLM推理绩效的作品之一。


23. Title: UniverSLU: Universal Spoken Language Understanding for Diverse  Classification and Sequence Generation Tasks with a Single Network
   Link: https://arxiv.org/abs/2310.02973
   Classification Answer: yes

   Abstract: Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model's behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model "UniverSLU" for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model's generalization capabilities to new paraphrases.

  Translated Abstract: 最近的研究表明，通过使用具有多任务功能的大型语言模型来表明有希望的结果。他们利用提示来指导模型的行为并超过特定于任务模型的性能。在此激励的情况下，我们问：我们可以建立一个共同执行各种口语理解（SLU）任务的单一模型吗？为了解决这个问题，我们利用预训练的自动语音识别（ASR）模型，并使用各种任务和数据集说明符作为离散提示。我们证明了我们的单个多任务学习（MTL）模型“ Universlu”的功效对于12种不同的语音分类和序列生成任务的功效。结果表明，Universlu实现了竞争性能，甚至超过了特定于任务的模型。我们还对实现人类解剖的自然短语而不是任务指定词来进行初步研究，作为离散提示并测试模型的概括能力，以对新释义。


24. Title: Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models
   Link: https://arxiv.org/abs/2310.02949
   Classification Answer: yes

   Abstract: Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5 different organizations (LLaMa-2, Falcon, InternLM, BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack. Besides, the single-turn English-only attack successfully transfers to multi-turn dialogue and other languages. This study serves as a clarion call for a collective effort to overhaul and fortify the safety of open-source LLMs against malicious attackers.

  Translated Abstract: 警告：本文包含有害语言的示例，建议读者酌情决定。强大的大语言模型（LLMS）的开放发布越来越多，通过降低数据注释和计算的基本成本来促进下游应用程序的开发。为了确保AI安全性，已经采取了广泛的安全 - 对准措施，以防止这些模型，以防止恶意使用（主要是硬及时攻击）。但是，在看似弹性的盔甲外墙下，可能会潜伏着阴影。通过简单地使用1 GPU小时调整100个恶意示例，这些安全对齐的LLM可以很容易地颠覆以产生有害内容。正式地，我们将新的攻击称为阴影对齐：使用少量数据可以引起安全平选的模型来适应有害任务而无需牺牲模型的帮助。值得注意的是，颠覆的模型保留了对定期查询做出适当响应的能力。5个不同组织（Llama-2，Falcon，Internlm，Baichuan2，Vicuna）发布的8个模型的实验证明了影子对齐攻击的有效性。此外，仅一折英语攻击成功地转移到多转话和其他语言。这项研究是一项克拉里昂呼吁，要求集体努力大修和加强开源LLMS针对恶意攻击者的安全。


25. Title: Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task  Adaptation
   Link: https://arxiv.org/abs/2310.02842
   Classification Answer: yes

   Abstract: Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new -- but often individual -- downstream tasks. Thus, how one would expand prompt tuning to handle -- concomitantly -- heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs, associated with smart gating functionality: the latter -- whose design is one of the contributions of this paper -- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied -- for efficiency reasons -- as well as instruction data source and task composition. In practice, MoPs can simultaneously mitigate prompt training "interference" in multi-task, multi-source scenarios (e.g., task and data heterogeneity across sources), as well as possible implications from model approximations. As a highlight, MoPs manage to decrease final perplexity from $\sim20\%$ up to $\sim70\%$, as compared to baselines, in the federated scenario, and from $\sim 3\%$ up to $\sim30\%$ in the centralized scenario.

  Translated Abstract: 大型语言模型（LLMS）具有解决各种任务的能力，例如文本摘要和数学问题，但通常会牢记他们的一项任务。由于计算成本的高，当前的趋势是使用及时的指令调整来更好地调整整体元素，预验证的LLM，以适应新的（但通常是个性化）的下游任务。因此，如何扩展及时的调整以处理 - 同时处理异质任务和数据分布是一个广泛的问题。为了解决这一差距，我们建议使用\ emph {提示的混合物}或与智能门控功能相关的拖把：后者 - 其设计是本文的贡献之一 - 可以识别嵌入不同的相关技能基于目标任务的提示和动态分配组合专家（即提示的收集）组。此外，出于效率原因，在经验上，MOP对任何模型压缩技术以及指导数据源和任务组成的应用都是不可知的。在实践中，MOP可以同时减轻多任务，多源场景（例如，跨源的任务和数据异质性）的迅速训练“干扰”，以及模型近似值的可能影响。作为突出显示，与基线相比，在联合场景中，拖把从$ \ sim20 \％$降低到$ \ sim70 \％$，从\％$在集中场景中。


26. Title: Low Resource Summarization using Pre-trained Language Models
   Link: https://arxiv.org/abs/2310.02790
   Classification Answer: yes

   Abstract: With the advent of Deep Learning based Artificial Neural Networks models, Natural Language Processing (NLP) has witnessed significant improvements in textual data processing in terms of its efficiency and accuracy. However, the research is mostly restricted to high-resource languages such as English and low-resource languages still suffer from a lack of available resources in terms of training datasets as well as models with even baseline evaluation results. Considering the limited availability of resources for low-resource languages, we propose a methodology for adapting self-attentive transformer-based architecture models (mBERT, mT5) for low-resource summarization, supplemented by the construction of a new baseline dataset (76.5k article, summary pairs) in a low-resource language Urdu. Choosing news (a publicly available source) as the application domain has the potential to make the proposed methodology useful for reproducing in other languages with limited resources. Our adapted summarization model \textit{urT5} with up to 44.78\% reduction in size as compared to \textit{mT5} can capture contextual information of low resource language effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore) at par with state-of-the-art models in high resource language English \textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed method provided a baseline approach towards extractive as well as abstractive summarization with competitive evaluation results in a limited resource setup.

  Translated Abstract: 随着基于深度学习的人工神经网络模型的出现，自然语言处理（NLP）在其效率和准确性方面见证了文本数据处理的显着改善。但是，这项研究主要仅限于高资源语言，例如英语和低资源语言，但在培训数据集以及甚至基线评估结果的模型方面仍然缺乏可用资源。考虑到低资源语言的资源可用性有限，我们提出了一种用于调整基于自动变压器的体系结构模型（Mbert，MT5）的方法，用于低资源摘要，并通过构建新的基线数据集（76.5k文章）来补充，摘要对）中低资源语言乌尔都语。选择新闻（公开可用来源）作为应用程序域，有可能使所提出的方法可用于以有限的资源来复制其他语言。与\ textit {mt5}相比，我们的改编的摘要模型\ textit {urt5}的大小降低了44.78 \％，可以有效地捕获低资源语言的上下文信息，并具有评估得分（最高46.35 Rouge-1，77 bertscore）与高资源语言英语\ textit {（Pegasus：47.21，Bart：45.14：XSUM DataSet上的45.14）中与最新模型相提并论。提出的方法为提取性和抽象性摘要提供了一种基线方法，并具有竞争性评估的结果，以有限的资源设置。


27. Title: A UMLS-Augmented Framework for Improving Factuality in Large Language  Models within Healthcare
   Link: https://arxiv.org/abs/2310.02778
   Classification Answer: yes

   Abstract: Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conducted blind reviews to evaluate the generated content, and the results indicate that this framework effectively enhances the factuality, completeness, and relevance of generated content. Our research demonstrates the effectiveness of using UMLS-augmented LLMs and highlights the potential application value of LLMs in in medical question-answering.

  Translated Abstract: 大型语言模型（LLM）表现出强大的文本生成功能，为医疗保健领域带来了前所未有的创新。尽管LLM对医疗保健中的应用具有巨大的希望，但将它们应用于实际临床方案带来了重大挑战，因为这些模型可能会产生偏离既定医疗事实甚至表现出潜在偏见的内容。在我们的研究中，我们基于统一的医学语言系统（UMLS）开发了一个增强的LLM框架，旨在更好地为医疗保健社区服务。我们使用Llama2-13b-Chat和Chatgpt-3.5作为我们的基准模型，并使用Rouge Score和BertScore进行自动评估，并在LIVEQA测试集中的104个问题上进行自动评估。此外，我们基于四个维度建立了医师评估的标准：事实，完整性，可读性和相关性。Chatgpt-3.5用于医师评估，其中有20个LiveQA测试集的问题。多位居民医师进行了盲目评估以评估生成的内容，结果表明，该框架有效地增强了生成内容的事实，完整性和相关性。我们的研究证明了使用UMLS授权的LLM的有效性，并突出了LLM在医疗问题中的潜在应用值。


28. Title: I$^2$KD-SLU: An Intra-Inter Knowledge Distillation Framework for  Zero-Shot Cross-Lingual Spoken Language Understanding
   Link: https://arxiv.org/abs/2310.02594
   Classification Answer: yes

   Abstract: Spoken language understanding (SLU) typically includes two subtasks: intent detection and slot filling. Currently, it has achieved great success in high-resource languages, but it still remains challenging in low-resource languages due to the scarcity of labeled training data. Hence, there is a growing interest in zero-shot cross-lingual SLU. Despite of the success of existing zero-shot cross-lingual SLU models, most of them neglect to achieve the mutual guidance between intent and slots. To address this issue, we propose an Intra-Inter Knowledge Distillation framework for zero-shot cross-lingual Spoken Language Understanding (I$^2$KD-SLU) to model the mutual guidance. Specifically, we not only apply intra-knowledge distillation between intent predictions or slot predictions of the same utterance in different languages, but also apply inter-knowledge distillation between intent predictions and slot predictions of the same utterance. Our experimental results demonstrate that our proposed framework significantly improves the performance compared with the strong baselines and achieves the new state-of-the-art performance on the MultiATIS++ dataset, obtaining a significant improvement over the previous best model in overall accuracy.

  Translated Abstract: 口语理解（SLU）通常包括两个子任务：意图检测和插槽填充。目前，它在高资源语言中取得了巨大的成功，但是由于贴有标签的培训数据的稀缺，在低资源语言中仍然具有挑战性。因此，对零拍的跨语性slu的兴趣越来越大。尽管现有的零击跨语性SLU模型取得了成功，但大多数人忽略了实现意图和插槽之间的相互指导。为了解决这个问题，我们为零击的跨语言语言理解（I $^2 $ kd-slu）提出了一个内部切口知识蒸馏框架，以建模相互指导。具体而言，我们不仅在不同语言中对同一话语的意图预测或插槽预测之间采用内部知识蒸馏，而且还采用了同一话语的意图预测和插槽预测之间的知识间蒸馏。我们的实验结果表明，与强基础相比，我们提出的框架可显着提高性能，并在Multiatis ++数据集上实现新的最新性能，从而在整体准确性上获得了对先前最佳模型的显着改善。


29. Title: CITING: Large Language Models Create Curriculum for Instruction Tuning
   Link: https://arxiv.org/abs/2310.02527
   Classification Answer: yes

   Abstract: The recent advancement of large language models (LLMs) has been achieved through a combo of instruction tuning and human alignment. However, building manually crafted instruction datasets and performing human alignment become the bottleneck for scaling the development of LLMs. In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs. Our method is inspired by how human students refine their writing skills by following the rubrics and learning from the revisions offered by their tutors. Specifically, we employ a teacher LLM to create a curriculum for instruction tuning of the student LLM, namely Curriculum Instruction TunING (CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics for evaluating the answers corresponding to various types of questions, and (2) the student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher. We further iteratively carry out it to embody the procedure of CITING. We compare CITING to a series of state-of-the-art baselines on four datasets. Our method demonstrates strong improvement in terms of articulate, in-depth, and comprehensive by GPT-4 evaluation. Specifically, it achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1% over RRHF, and 76.3% over RAFT, respectively.

  Translated Abstract: 通过教学调整和人类的一致性组合，已经实现了大型语言模型（LLM）的最新进步。但是，构建手动制作的指令数据集和执行人类对齐方式成为扩展LLM开发的瓶颈。在本文中，我们利用利用AI模型代替人类作为老师培训学生LLM的想法。我们的方法的灵感来自于人类学生如何通过遵循标题并从其导师提供的修订中学习来提高自己的写作技巧。具体来说，我们采用教师LLM来创建一个课程，以调整学生LLM的教学，即课程教学调整（引用）。它涵盖了两个主要步骤：（1）老师llm crafts评估与各种问题相对应的答案的专栏，以及（2）学生LLM学会遵循专栏并从老师进行的修订中进行自我纠正。我们进一步迭代地进行它以体现引用的过程。我们将引用与四个数据集上的一系列最先进的基线进行了比较。通过GPT-4评估，我们的方法在表达，深入和全面的方面表现出了很大的改善。具体而言，它的平均获胜率比SFT的平均获胜率为79.4％，超过RLHF的73.4％，RRHF超过78.1％，比RAFT分别为76.3％。


30. Title: ResidualTransformer: Residual Low-rank Learning with Weight-sharing for  Transformer Layers
   Link: https://arxiv.org/abs/2310.02489
   Classification Answer: yes

   Abstract: Memory constraint of always-on devices is one of the major concerns when deploying speech processing models on these devices. While larger models trained with sufficiently large amount of data generally perform better, making them fit in the device memory is a demanding challenge. In this paper, we aim to reduce model size by reparameterizing model weights across Transformer encoder layers and assuming a special weight composition and structure. More specifically, inspired by ResNet and the more recent LoRA work, we propose an approach named ResidualTransformer, where each weight matrix in a Transformer layer comprises 1) a shared full-rank component with its adjacent layers, and 2) a unique low-rank component to itself. The low-rank matrices only account for a small amount of model size increase. In addition, we add diagonal weight matrices to improve modeling capacity of the low-rank matrices. Experiments of our 10k-hour speech recognition and speech translation tasks show that the Transformer encoder size can be reduced by ~3X with very slight performance degradation.

  Translated Abstract: 在这些设备上部署语音处理模型时，始终对设备的内存约束是主要问题之一。虽然经过足够大量数据训练的较大型号通常表现得更好，但使其适合设备内存是一个挑战的挑战。在本文中，我们旨在通过对变压器编码层进行重新聚集的模型权重来减少模型尺寸，并假设具有特殊的权重组成和结构。更具体地说，是受重新系统启发和较新的洛拉（Lora）作品的启发，我们提出了一种名为“残留变形物本身的组成部分。低级矩阵仅考虑少量模型尺寸的增加。此外，我们添加了对角重量矩阵，以提高低级矩阵的建模能力。我们10k小时的语音识别和语音翻译任务的实验表明，变压器编码器的大小可以减少〜3倍，而性能很小。


31. Title: Large Language Models Can Be Good Privacy Protection Learners
   Link: https://arxiv.org/abs/2310.02469
   Classification Answer: yes

   Abstract: The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, effectively protecting private data while enhancing the model's knowledge. Our work underscores the potential for Large Language Models as robust privacy protection learners.

  Translated Abstract: 大型语言模型（LLMS）的扩散引起了人们对使用特定领域的数据进行微调以创建专业语言模型的浓厚兴趣。然而，这种特定领域的微调数据通常包含敏感的个人身份信息（PII）。在没有隐私保护的情况下，直接在此数据上进行微调LLM会带来泄漏的风险。为了应对这一挑战，我们引入了隐私保护语言模型（PPLM），这是一种用于微调LLM的新型范式，可有效地注入特定于领域的知识，同时保护数据隐私。我们的工作为模型设计提供了理论分析，并研究了各种技术，例如语料库策划，基于惩罚的训练损失的不可能，以及基于教学的调整等。各种数据集和场景的广泛实验证明了我们方法的有效性。特别是，用正面和负面示例进行教学调整是一种有前途的方法，可以有效地保护私人数据，同时增强模型的知识。我们的工作强调了大型语言模型作为强大的隐私保护学习者的潜力。


32. Title: The Empty Signifier Problem: Towards Clearer Paradigms for  Operationalising "Alignment" in Large Language Models
   Link: https://arxiv.org/abs/2310.02457
   Classification Answer: yes

   Abstract: In this paper, we address the concept of "alignment" in large language models (LLMs) through the lens of post-structuralist socio-political theory, specifically examining its parallels to empty signifiers. To establish a shared vocabulary around how abstract concepts of alignment are operationalised in empirical datasets, we propose a framework that demarcates: 1) which dimensions of model behaviour are considered important, then 2) how meanings and definitions are ascribed to these dimensions, and by whom. We situate existing empirical literature and provide guidance on deciding which paradigm to follow. Through this framework, we aim to foster a culture of transparency and critical evaluation, aiding the community in navigating the complexities of aligning LLMs with human populations.

  Translated Abstract: 在本文中，我们通过后结构主义社会政治理论的角度解决了大语言模型（LLM）中“对齐”的概念，专门研究了其与空的指示符的相似之处。为了建立一个共同的词汇，围绕在经验数据集中如何实现对齐方式的抽象概念，我们提出了一个划定划分的框架：1）模型行为的哪个维度被认为是重要的，然后是2）2）如何将含义和定义归因于这些维度以及这些维度。谁。我们将现有的经验文献放置，并为决定遵循哪些范式提供指导。通过这个框架，我们旨在促进透明度和批判性评估的文化，以帮助社区导航LLM与人类人群的复杂性。


33. Title: Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of  Large Language Models with Misconceptions
   Link: https://arxiv.org/abs/2310.02439
   Classification Answer: yes

   Abstract: We propose novel evaluations for mathematical reasoning capabilities of Large Language Models (LLMs) based on mathematical misconceptions. Our primary approach is to simulate LLMs as a novice learner and an expert tutor, aiming to identify the incorrect answer to math question resulted from a specific misconception and to recognize the misconception(s) behind an incorrect answer, respectively. Contrary to traditional LLMs-based mathematical evaluations that focus on answering math questions correctly, our approach takes inspirations from principles in educational learning sciences. We explicitly ask LLMs to mimic a novice learner by answering questions in a specific incorrect manner based on incomplete knowledge; and to mimic an expert tutor by identifying misconception(s) corresponding to an incorrect answer to a question. Using simple grade-school math problems, our experiments reveal that, while LLMs can easily answer these questions correctly, they struggle to identify 1) the incorrect answer corresponding to specific incomplete knowledge (misconceptions); 2) the misconceptions that explain particular incorrect answers. Our study indicates new opportunities for enhancing LLMs' math reasoning capabilities, especially on developing robust student simulation and expert tutoring models in the educational applications such as intelligent tutoring systems.

  Translated Abstract: 我们根据数学误解提出了针对大语言模型（LLM）数学推理能力的新评估。我们的主要方法是模拟LLM作为新手学习者和专家导师，旨在确定由于特定的误解而产生的数学问题的错误答案，并分别认识到错误答案背后的误解。与传统的基于LLMS的数学评估相反，该评估的重点是正确回答数学问题，我们的方法从教育学习科学的原则中汲取了灵感。我们通过基于不完整的知识以特定的不正确方式回答问题来明确要求LLMS模仿新手学习者；并通过确定与问题不正确的答案相对应的误解来模仿专家教师。使用简单的成绩数学问题，我们的实验表明，尽管LLM可以轻松地正确回答这些问题，但它们很难识别1）与特定不完整知识相对应的错误答案（误解）；2）解释特定错误答案的误解。我们的研究表明了增强LLMS数学推理功能的新机会，尤其是在开发健壮的学生模拟和专家辅导模型（例如智能辅导系统）中。


34. Title: Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only  Language Models
   Link: https://arxiv.org/abs/2310.02409
   Classification Answer: yes

   Abstract: Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin & Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed "nuggets" which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.

  Translated Abstract: 基于标准变压器的语言模型（LMS）的规模较差到长上下文。我们提出了一个基于动态上下文压缩的解决方案，该解决方案将Qin＆van Durme（2023）的掘金方法从类似Bert的框架到仅解码器的LMS。我们的方法模型历史记录是被训练以允许重建的压缩“掘金”，并且可以使用现成的模型（例如Llama）进行初始化。我们通过在语言建模，问题回答和摘要方面的实验中证明了Nugget2D在这些任务中保留功能，同时在时间和空间方面大大减少了开销。例如，在自动编码的实验中，Nugget2D可以以20倍的压缩比收缩上下文，重建的BLEU得分为98％，获得了几乎无损编码。


35. Title: Conversational Health Agents: A Personalized LLM-Powered Agent Framework
   Link: https://arxiv.org/abs/2310.02374
   Classification Answer: yes

   Abstract: Conversational Health Agents (CHAs) are interactive systems designed to enhance personal healthcare services by engaging in empathetic conversations and processing multimodal data. While current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation, they often lack comprehensive agent capabilities. This includes the ability to access personal user health data from wearables, 24/7 data collection sources, and electronic health records, as well as integrating the latest published health insights and connecting with established multimodal data analysis tools. We are developing a framework to empower CHAs by equipping them with critical thinking, knowledge acquisition, and problem-solving abilities. Our CHA platform, powered by LLMs, seamlessly integrates healthcare tools, enables multilingual and multimodal conversations, and interfaces with a variety of user data analysis tools. We illustrate its proficiency in handling complex healthcare tasks, such as stress level estimation, showcasing the agent's cognitive and operational capabilities.

  Translated Abstract: 对话卫生代理（CHAS）是互动系统，旨在通过参与善解人意的对话和处理多模式数据来增强个人医疗服务。尽管当前的CHA，特别是利用大型语言模型（LLM）的CHA，主要集中在对话上，但他们通常缺乏全面的代理能力。这包括从可穿戴设备，24/7数据收集来源和电子健康记录中访问个人用户健康数据的能力，以及整合最新的已发表的健康见解并与已建立的多模式数据分析工具连接。我们正在开发一个框架来通过为他们提供批判性思维，知识获取和解决问题的能力来增强CHA的能力。我们的CHA平台由LLMS提供支持，无缝整合了医疗保健工具，启用了多语言和多模式对话，以及与各种用户数据分析工具的接口。我们说明了它在处理复杂的医疗任务方面的熟练程度，例如压力水平估计，展示了代理商的认知和操作能力。


36. Title: On the definition of toxicity in NLP
   Link: https://arxiv.org/abs/2310.02357
   Classification Answer: yes

   Abstract: The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. This causes us to rely on subjective and vague data in models' training, which results in non-robust and non-accurate results: garbage in - garbage out.
This work suggests a new, stress-level-based definition of toxicity designed to be objective and context-aware. On par with it, we also describe possible ways of applying this new definition to dataset creation and model training.

  Translated Abstract: 毒性检测任务的基本问题在于毒性不明显。这使我们依靠模型培训中的主观和模糊数据，从而导致不持胸花和不准确的结果：垃圾中的垃圾。
这项工作提出了一个新的，基于压力水平的毒性定义，旨在客观和背景感知。与此相同，我们还描述了将此新定义应用于数据集创建和模型培训的可能方法。


37. Title: Understanding In-Context Learning in Transformers and LLMs by Learning  to Learn Discrete Functions
   Link: https://arxiv.org/abs/2310.03016
   Classification Answer: yes

   Abstract: In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can learn gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided a teaching sequence, i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implement two distinct algorithms to solve a single task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples. (c) Lastly, we show that extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines on prediction tasks that are guaranteed to not be in their training set.

  Translated Abstract: 为了理解秘密学习现象，最近的作品采用了风格化的实验框架，并证明了变压器可以学习基于梯度的学习算法，以了解各种相同价值的功能。但是，变形金刚在实施学习算法中的局限性及其学习其他形式的算法的能力尚不清楚。此外，这些功能仅限于基于注意力的模型的程度尚不清楚。此外，是否可以将这些风格化设置得出的见解可以推断到预处理的大语言模型（LLMS）尚待观察。在这项工作中，我们通过演示以下内容迈出了回答这些问题的一步：（a）在带有各种布尔功能类的测试床上，我们发现变形金刚几乎可以匹配“简单”任务的最佳学习算法，而他们的性能在更“复杂”的任务上恶化。此外，我们发现某些无注意的模型（几乎）与一系列任务上的变压器相同（几乎）。（b）当提供了教学顺序时，即一组独特地识别类中函数的示例时，我们表明变形金刚更有效地学习了样本。有趣的是，我们的结果表明，变形金刚可以学会实现两种不同的算法来求解单个任务，并且可以根据文本示例的序列自适应地选择更效率的算法。（c）最后，我们表明现存的LLM，例如Llama-2，GPT-4可以与最近的邻居基线竞争预测任务，这些任务保证不在训练中。


38. Title: Kosmos-G: Generating Images in Context with Multimodal Large Language  Models
   Link: https://arxiv.org/abs/2310.02992
   Classification Answer: yes

   Abstract: Recent advancements in text-to-image (T2I) and vision-language-to-image (VL2I) generation have made significant strides. However, the generation from generalized vision-language inputs, especially involving multiple images, remains under-explored. This paper presents Kosmos-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of "image as a foreign language in image generation."

  Translated Abstract: 文本到图像（T2I）和视觉语言对图像（VL2I）的最新进展已取得了长足的进步。但是，从广义视力语言输入（尤其是涉及多个图像）的产生仍未得到探索。本文介绍了Kosmos-G，该模型利用多模式大型语言模型（MLLM）的高级感知能力来应对上述挑战。我们的方法将MLLM的输出空间与剪辑相一致，以文本方式作为锚点，并在策划数据上进行组成指令调整。kosmos-g展示了零击多实体主题驱动的一代的独特功能。值得注意的是，得分蒸馏指令调整不需要对图像解码器进行任何修改。这允许无缝替代剪辑和轻松地集成，并与无数的U-NET技术从细粒度控件到个性化的图像解码器变体。我们认为Kosmos-G是最初的尝试，以“图像生成中的外语”的目标。


39. Title: xVal: A Continuous Number Encoding for Large Language Models
   Link: https://arxiv.org/abs/2310.02989
   Classification Answer: yes

   Abstract: Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.

  Translated Abstract: 大型语言模型尚未广泛适应用于分析有关令牌数字的独特困难的科学数据集的分析。我们提出了Xval，这是一种数值编码方案，仅使用一个令牌来表示任何实际数字。XVAL通过按数字值缩放专用嵌入向量来表示给定的实际数字。结合修改后的数字指示方法，此策略将模型端到端连续呈现为从输入字符串的映射到输出字符串的映射。这会导致归纳偏置，通常更适合于科学领域的应用。我们经验评估了有关许多合成和现实数据集的建议。与现有的编码方案相比，我们发现XVAL具有更高的令牌，并证明了改善的概括。


40. Title: Never Train from Scratch: Fair Comparison of Long-Sequence Models  Requires Data-Driven Priors
   Link: https://arxiv.org/abs/2310.02980
   Classification Answer: yes

   Abstract: Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.

  Translated Abstract: 在序列之间建模长期依赖性是机器学习的一个长期目标，并导致了诸如状态空间模型之类的体系结构，在长序列上极大地超过了变压器。但是，这些令人印象深刻的经验收益在基准（例如远距离竞技场）上进行了很大的证明，其中模型是随机初始初始化和训练以从输入序列预测目标标签的。在这项工作中，我们表明随机初始化会导致对体系结构之间的差异的高估和使用$ \ textIt {仅下游任务数据} $进行标准降级目标进行预处理，从而导致多个架构之间的巨大增长和非常小的小型增长变压器和状态空间模型（SSM）之间的差距。与先前的作品形成鲜明对比的是，我们发现在正确鉴定的远距离竞技场上，香草变压器可以与S4的性能相匹配，并且我们将SSM在Pathx-256任务上的最佳报告结果提高了20个绝对点。随后，我们分析了SSM的先前提供的结构化参数化的效用，并表明它们在通过预处理获得的数据驱动初始化的情况下大多变得多余。我们的工作表明，在评估监督任务的不同体系结构时，通过预训练将数据驱动的先验合并对于可靠的绩效估算至关重要，并且可以有效地进行。


41. Title: Improving Automatic VQA Evaluation Using Large Language Models
   Link: https://arxiv.org/abs/2310.02567
   Classification Answer: yes

   Abstract: 8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We hope wide adoption of our metric will contribute to better estimating the research progress on the VQA task.

  Translated Abstract: 提出了视觉问答（VQA）任务8年后，准确性仍然是自动评估的主要指标。在IID评估设置中，VQA准确性已经有效。但是，我们的社区正在转向开放式生成模型和OOD评估。在这个新的范式中，现有的VQA精度度量标准过于严格，低估了VQA系统的性能。因此，有必要开发更强大的自动VQA指标，以替代人类判断。在这项工作中，我们建议利用教学调节的大语模型（LLM）的文化学习能力来构建更好的VQA指标。我们将VQA评估作为一个答案评估任务，其中指示LLM给出一组参考答案的候选人答案的准确性。与几种VQA模型和基准的现有指标相比，我们证明了所提出的指标与人类判断更好。我们希望我们的指标广泛采用将有助于更好地估计VQA任务的研究进度。


42. Title: Can Large Language Models Provide Security & Privacy Advice? Measuring  the Ability of LLMs to Refute Misconceptions
   Link: https://arxiv.org/abs/2310.02431
   Classification Answer: yes

   Abstract: Users seek security & privacy (S&P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&P advice is not well-explored. In this paper, we measure their ability to refute popular S&P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to these misconceptions. To comprehensively evaluate their responses, we further apply three strategies: query each misconception multiple times, generate and query their paraphrases, and solicit source URLs of the responses. Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate increases to 32.6% when we repeatedly query LLMs with the same or paraphrased misconceptions. We also expose that models may partially support a misconception or remain noncommittal, refusing a firm stance on misconceptions. Our exploration of information sources for responses revealed that LLMs are susceptible to providing invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to unrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).

  Translated Abstract: 用户从在线资源（包括受信任的网站和内容共享平台）中寻求安全与隐私（S＆P）建议。这些资源可帮助用户了解标准普尔技术和工具，并提出可行的策略。大型语言模型（LLMS）最近作为可信赖的信息来源出现。但是，他们的准确性和正确性受到质疑。先前的研究概述了LLM在回答多项选择的问题和用户无意间规避模型限制的能力方面的缺点（例如，产生有毒内容）。但是，LLM提供可靠的标准普尔建议的能力并未得到充分探索。在本文中，我们衡量了他们驳斥公众所拥有的流行的标准普尔误解的能力。我们首先研究了最近的学术文献，以策划六个不同主题的一百多个标准普尔相关误解的数据集。然后，我们查询两个受欢迎的LLM（吟游诗人和Chatgpt），并制定标签指南，以评估他们对这些误解的反应。为了全面评估他们的反应，我们进一步采用了三种策略：多次查询每个误解，生成和查询它们的释义，并征求响应的来源URL。两种模型平均表明，不可忽略的错误率平均为21.3％，错误地支持流行的标准普尔误解。当我们反复以相同或解释误解的方式查询LLM时，错误率将增加到32.6％。我们还表明，模型可能会部分支持误解或保持不可交易，拒绝对误解的坚定立场。我们对响应的信息来源的探索表明，LLMS很容易提供无效的URL（bard 21.2％，而Chatgpt的67.7％）或指向无关的来源（Bard返回的44.2％，Chatgpt返回18.3％）。


43. Title: Can a student Large Language Model perform as well as it's teacher?
   Link: https://arxiv.org/abs/2310.02421
   Classification Answer: yes

   Abstract: The burgeoning complexity of contemporary deep learning models, while achieving unparalleled accuracy, has inadvertently introduced deployment challenges in resource-constrained environments. Knowledge distillation, a technique aiming to transfer knowledge from a high-capacity "teacher" model to a streamlined "student" model, emerges as a promising solution to this dilemma. This paper provides a comprehensive overview of the knowledge distillation paradigm, emphasizing its foundational principles such as the utility of soft labels and the significance of temperature scaling. Through meticulous examination, we elucidate the critical determinants of successful distillation, including the architecture of the student model, the caliber of the teacher, and the delicate balance of hyperparameters. While acknowledging its profound advantages, we also delve into the complexities and challenges inherent in the process. Our exploration underscores knowledge distillation's potential as a pivotal technique in optimizing the trade-off between model performance and deployment efficiency.

  Translated Abstract: 当代深度学习模型的蓬勃发展的复杂性虽然达到了无与伦比的准确性，但在资源受限的环境中无意中引入了部署挑战。知识蒸馏是一种旨在将知识从高容量的“教师”模型转移到简化的“学生”模型的技术，它是解决这一困境的有前途的解决方案。本文提供了知识蒸馏范式的全面概述，强调了其基本原理，例如软标签的实用性和温度缩放的重要性。通过细致的检查，我们阐明了成功蒸馏的关键决定因素，包括学生模型的建筑，教师的才能以及超参数的微妙平衡。在承认它的深刻优势的同时，我们还深入研究了此过程中固有的复杂性和挑战。我们的探索强调了知识蒸馏作为优化模型性能和部署效率之间的权衡的关键技术的潜力。


44. Title: Contrastive Post-training Large Language Models on Data Curriculum
   Link: https://arxiv.org/abs/2310.02263
   Classification Answer: yes

   Abstract: Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we explore contrastive post-training techniques for alignment by automatically constructing preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We carefully compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continueing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from "easier" pairs and transitioning to "harder" ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to exceed that of ChatGPT.

  Translated Abstract: 对齐是将大型语言模型（LLM）转向人类偏好的重要一步。在本文中，我们通过自动构建各种强度的模型（例如，指令gpt，chatgpt和gpt-4）来探索对比后的训练后训练技术来对齐。我们仔细地将SLIC和DPO的对比技术与SFT基准进行了比较，并发现DPO即使在持续SFT饱和后，DPO也提供了逐步的改进。我们还探索了一种用于对比后培训的数据课程学习方案，该方案首先要从“更轻松”的对并过渡到“更难”的学习课程，从而进一步改善了对齐。最后，我们扩大实验，以训练更多的数据和更大的模型，例如ORCA。值得注意的是，对比后的训练进一步改善了ORCA的性能，ORCA已经是使用GPT-4输出调整的最先进的指导学习模型，超过了CHATGPT。


45. Title: Harnessing Pre-Trained Sentence Transformers for Offensive Language  Detection in Indian Languages
   Link: https://arxiv.org/abs/2310.02249
   Classification Answer: yes

   Abstract: In our increasingly interconnected digital world, social media platforms have emerged as powerful channels for the dissemination of hate speech and offensive content. This work delves into the domain of hate speech detection, placing specific emphasis on three low-resource Indian languages: Bengali, Assamese, and Gujarati. The challenge is framed as a text classification task, aimed at discerning whether a tweet contains offensive or non-offensive content. Leveraging the HASOC 2023 datasets, we fine-tuned pre-trained BERT and SBERT models to evaluate their effectiveness in identifying hate speech. Our findings underscore the superiority of monolingual sentence-BERT models, particularly in the Bengali language, where we achieved the highest ranking. However, the performance in Assamese and Gujarati languages signifies ongoing opportunities for enhancement. Our goal is to foster inclusive online spaces by countering hate speech proliferation.

  Translated Abstract: 在我们越来越相互联系的数字世界中，社交媒体平台已成为传播仇恨言论和冒犯内容的强大渠道。这项工作深入研究了仇恨言论检测的领域，特别强调了三种低资源的印度语言：孟加拉语，阿萨姆语和古吉拉特语。挑战是作为文本分类任务构成的，旨在辨别推文是否包含进攻性或非犯罪内容。利用HASOC 2023数据集，我们对预先训练的Bert和Sbert模型进行了微调，以评估它们在识别仇恨言论方面的有效性。我们的发现强调了单语句子模型的优越性，尤其是在孟加拉语中，我们获得了最高排名。但是，Assamese和Gujarati语言的表现意味着正在进行的增强机会。我们的目标是通过反对仇恨言论扩散来培养包容性的在线空间。


46. Title: Who's Harry Potter? Approximate Unlearning in LLMs
   Link: https://arxiv.org/abs/2310.02238
   Classification Answer: yes

   Abstract: Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch.
We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models.
Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model's own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model's memory whenever it is prompted with its context.

  Translated Abstract: 大型语言模型（LLM）经过大量互联网语料库的培训，这些语料库通常包含受版权保护的内容。这对这些模型的开发人员和用户以及原始作者和出版商构成了法律和道德挑战。在本文中，我们提出了一种新型技术，用于从LLM中学习训练数据的子集，而不必从头开始重新训练。
我们评估了我们从Llama2-7b模型（最近由Meta开源的生成语言模型）学习Harry Potter Books的任务。虽然该模型花费了超过184K的GPU小时才能预算，但我们表明，在大约1 GPU的填充小时中，我们有效地消除了该模型生成或回忆哈利·波特相关内容的能力，而其在常见基准（例如Winogrande，Winogrande，例如Winogrande，例如Winogrande，例如Winogrande）上的能力Hellaswag，Arc，Boolq和Piqa）几乎不受影响。我们使我们的微调模型在拥抱面上公开可用，以进行社区评估。据我们所知，这是第一篇论文介绍在生成语言模型中学习的有效技术。
我们的技术由三个主要组成部分组成：首先，我们使用一个增强模型，该模型在目标数据上进一步训练，以通过将其逻辑与基线模型的逻辑进行比较来识别与未学习目标最相关的令牌。其次，我们用通用对应物替换目标数据中的特质表达式，并利用模型自己的预测来为每个令牌生成替代标签。这些标签旨在近似未经目标数据训练的模型的下一步预测。第三，我们对这些替代标签上的模型进行了修订，每当以其上下文提示，它们会从模型的内存中有效地擦除原始文本。


47. Title: Think before you speak: Training Language Models With Pause Tokens
   Link: https://arxiv.org/abs/2310.02226
   Classification Answer: yes

   Abstract: Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\%$ EM score on the QA task of SQuAD, $8\%$ on CommonSenseQA and $1\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.

  Translated Abstract: 语言模型通过立即继承产生一系列令牌来产生响应：$（k+1）^{th} $令牌是操纵$ k $ hidden vectors每层的结果，每个载体一个向量是每个上述令牌。相反，我们要让模型操纵说，$ k+10 $隐藏的向量在输出$（k+1）^{th} $令牌之前，该怎么办？我们通过使用（可学习的）$ \ textIt {pause} $令牌对语言模型进行培训和推断来实现这个想法，该序列附加到输入前缀。然后，我们延迟提取模型的输出，直到看到最后一个暂停令牌，从而允许该模型在提交答案之前处理额外的计算。我们在C4上的因果预处理以及涵盖推理，提问，一般理解和事实回忆的下游任务上，对1B和13000万参数的$ \ textit {暂停培训} $进行了$ \ textit {暂停培训} $。我们的主要发现是，当模型经过预先训练和对延迟进行填充时，推理时间延迟显示出收益。对于1B型号，我们目睹了9个任务中的8项收益，最突出的是，在Squad的QA任务上获得了$ 18 \％$ EM的分数，CommonSenseQA的$ 8 \％$ $ $ \％$ $ 1 \％\％\％$ $ $ $ $ $ $ $ $ $准确性GSM8K。我们的工作提出了一系列概念和实用的研究问题，即使延迟下一步的预测成为广泛适用的新范式。


48. Title: Can Language Models be Instructed to Protect Personal Information?
   Link: https://arxiv.org/abs/2310.02224
   Classification Answer: yes

   Abstract: Large multimodal language models have proven transformative in numerous applications. However, these models have been shown to memorize and leak pre-training data, raising serious user privacy and information security concerns. While data leaks should be prevented, it is also crucial to examine the trade-off between the privacy protection and model utility of proposed approaches. In this paper, we introduce PrivQA -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario. We also propose a technique to iteratively self-moderate responses, which significantly improves privacy. However, through a series of red-teaming experiments, we find that adversaries can also easily circumvent these protections with simple jailbreaking methods through textual and/or image inputs. We believe PrivQA has the potential to support the development of new models with improved privacy protections, as well as the adversarial robustness of these protections. We release the entire PrivQA dataset at this https URL.

  Translated Abstract: 大型多模式模型在众多应用中已被证明具有变革性。但是，这些模型已被证明是为了记住和泄漏预培训数据，从而提高了严重的用户隐私和信息安全问题。尽管应防止数据泄漏，但研究拟议方法的隐私保护和模型效用之间的权衡也至关重要。在本文中，我们介绍了PrivQA-当指示模型在模拟场景中保护特定类别的个人信息类别时，可以评估本隐私/公用事业权衡的多模式基准。我们还为迭代自我中等反应提出了一种技术，从而大大改善了隐私。但是，通过一系列红色团队的实验，我们发现对手也可以通过文本和/或图像输入轻松地通过简单的越狱方法来规避这些保护措施。我们认为，Privqa有潜力支持通过改进的隐私保护以及这些保护的对抗性鲁棒性的新模型的开发。我们在此HTTPS URL上发布了整个PrivQA数据集。


49. Title: Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
   Link: https://arxiv.org/abs/2310.02174
   Classification Answer: yes

   Abstract: With the emergence of generative conversational large language models (LLMs) like ChatGPT, serving as virtual assistants in various fields, the stability and reliability of their responses have become crucial. However, during usage, it has been observed that these models tend to waver in their judgements when confronted with follow-up questions from users expressing skepticism or disagreement. In this work, we draw inspiration from questioning strategies in education and propose a \textsc{Follow-up Questioning Mechanism} along with two evaluation metrics to assess the judgement consistency of LLMs before and after exposure to disturbances. We evaluate the judgement consistency of ChatGPT, PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning benchmarks. Empirical results show that even when the initial answers are correct, judgement consistency sharply decreases when LLMs face disturbances such as questioning, negation, or misleading. Additionally, we study these models' judgement consistency under various settings (sampling temperature and prompts) to validate this issue further, observing the impact of prompt tone and conducting an in-depth error analysis for deeper behavioral insights. Furthermore, we also explore several prompting methods to mitigate this issue and demonstrate their effectiveness\footnote{\url{this https URL}}.

  Translated Abstract: 随着生成性对话大语言模型（LLM）的出现，例如Chatgpt，是各个领域的虚拟助手，其响应的稳定性和可靠性变得至关重要。但是，在使用期间，人们观察到，当面对表达怀疑或分歧的用户的后续问题时，这些模型往往会动摇判断。在这项工作中，我们从质疑教育策略的质疑中汲取灵感，并提出\ textsc {后续询问机制}以及两个评估指标，以评估暴露于骚扰之前和之后LLM的判断一致性。我们在八个推理基准中评估了Chatgpt，Palm2-Bison和Vicuna-13b的判断一致性。经验结果表明，即使初始答案正确，判断一致性也会急剧下降，而LLM面临诸如质疑，否定或误导之类的干扰。此外，我们研究了这些模型在各种设置（采样温度和提示）下的判断一致性，以进一步验证此问题，观察及时音调的影响并进行深入的误差分析，以实现更深入的行为见解。此外，我们还探索了一些提示方法来减轻此问题并演示其有效性\ footNote {\ url {this HTTPS url}}。


50. Title: Large Language Models Meet Knowledge Graphs to Answer Factoid Questions
   Link: https://arxiv.org/abs/2310.02166
   Classification Answer: yes

   Abstract: Recently, it has been shown that the incorporation of structured knowledge into Large Language Models significantly improves the results for a variety of NLP tasks. In this paper, we propose a method for exploring pre-trained Text-to-Text Language Models enriched with additional information from Knowledge Graphs for answering factoid questions. More specifically, we propose an algorithm for subgraphs extraction from a Knowledge Graph based on question entities and answer candidates. Then, we procure easily interpreted information with Transformer-based models through the linearization of the extracted subgraphs. Final re-ranking of the answer candidates with the extracted information boosts Hits@1 scores of the pre-trained text-to-text language models by 4-6%.

  Translated Abstract: 最近，已经表明，将结构化知识纳入大语言模型可以显着改善各种NLP任务的结果。在本文中，我们提出了一种探索预先训练的文本到文本语言模型的方法，这些语言模型丰富了知识图中的其他信息，以回答Factoid问题。更具体地说，我们提出了一种基于问题实体并回答候选者从知识图中提取子图的算法。然后，我们通过提取的子图的线性化来采购具有基于变压器的模型的易于解释的信息。通过提取的信息将答案候选者的最终重新排列提高了@1个预训练的文本对文本语言模型的命中率为4-6％。


51. Title: Unveiling the Pitfalls of Knowledge Editing for Large Language Models
   Link: https://arxiv.org/abs/2310.02129
   Classification Answer: yes

   Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of LLMs. Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works. Code will be released at this https URL.

  Translated Abstract: 随着与微调大语言模型（LLM）相关的成本不断上升，最近的研究工作涉及开发方法，以编辑LLM中嵌入的隐性知识。但是，仍然有乌云在头顶上徘徊 - 知识编辑会触发蝴蝶效应吗？由于尚不清楚知识编辑是否会引入副作用，从而带来潜在的风险。本文开创了对与LLMS知识编辑相关的潜在陷阱的调查。为了实现这一目标，我们介绍了新的基准数据集并提出创新的评估指标。我们的结果强调了两个关键问题：（1）知识冲突：逻辑上冲突可以放大llms-a方面的固有不一致的事实组的编辑组，而先前方法却忽略了。（2）知识失真：改变参数以编辑事实知识的目的可以不可撤销地扭曲LLM的先天知识结构。实验结果生动地表明，知识编辑可能会无意中对LLM产生意想不到的后果的阴影，LLM值得关注和对未来作品的努力。代码将在此HTTPS URL上发布。


52. Title: Instance Needs More Care: Rewriting Prompts for Instances Yields Better  Zero-Shot Performance
   Link: https://arxiv.org/abs/2310.02107
   Classification Answer: yes

   Abstract: Enabling large language models (LLMs) to perform tasks in zero-shot has been an appealing goal owing to its labor-saving (i.e., requiring no task-specific annotations); as such, zero-shot prompting approaches also enjoy better task generalizability. To improve LLMs' zero-shot performance, prior work has focused on devising more effective task instructions (e.g., ``let's think step by step'' ). However, we argue that, in order for an LLM to solve them correctly in zero-shot, individual test instances need more carefully designed and customized instructions. To this end, we propose PRoMPTd, an approach that rewrites the task prompt for each individual test input to be more specific, unambiguous, and complete, so as to provide better guidance to the task LLM. We evaluated PRoMPTd on eight datasets covering tasks including arithmetics, logical reasoning, and code generation, using GPT-4 as the task LLM. Notably, \algoname achieves an absolute improvement of around 10\% on the complex MATH dataset and 5\% on the code generation task on HumanEval, outperforming conventional zero-shot methods. In addition, we also showed that the rewritten prompt can provide better interpretability of how the LLM resolves each test instance, which can potentially be leveraged as a defense mechanism against adversarial prompting. The source code and dataset can be obtained from this https URL

  Translated Abstract: 使大型语言模型（LLMS）能够在零拍摄中执行任务，这是一个有吸引力的目标，这是由于其避免了劳动力（即不需要特定于任务的注释）；因此，零射击提示方法也享有更好的任务概括性。为了提高LLMS的零拍摄性能，先前的工作重点是设计更有效的任务说明（例如，``````逐步''）。但是，我们认为，为了使LLM在零拍摄中正确解决它们，单个测试实例需要更仔细的设计和定制指令。为此，我们提出了提示，该方法可以重写每个单独的测试输入的任务提示，使得更具体，明确和完整，以便为任务llm提供更好的指导。我们使用GPT-4作为任务LLM评估了八个数据集上的提示，其中包括算术，逻辑推理和代码生成。值得注意的是，\ algoname在复杂的数学数据集上实现了约10 \％的绝对提高，而在HOMANEVAL上的代码生成任务上的5 \％都超过了常规的零照片。此外，我们还表明，重写的提示可以更好地解释LLM如何解决每个测试实例，该实例有可能将其作为防御机制的防御机制，以防止对抗性提示。可以从此HTTPS URL获得源代码和数据集


53. Title: Tuning Large language model for End-to-end Speech Translation
   Link: https://arxiv.org/abs/2310.02050
   Classification Answer: yes

   Abstract: With the emergence of large language models (LLMs), multimodal models based on LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM, and SpeechGPT exhibit an impressive ability to comprehend and generate human instructions. However, their performance often falters when faced with complex tasks like end-to-end speech translation (E2E-ST), a cross-language and cross-modal translation task. In comparison to single-modal models, multimodal models lag behind in these scenarios. This paper introduces LST, a Large multimodal model designed to excel at the E2E-ST task. LST consists of a speech frontend, an adapter, and a LLM backend. The training of LST consists of two stages: (1) Modality adjustment, where the adapter is tuned to align speech representation with text embedding space, and (2) Downstream task fine-tuning, where both the adapter and LLM model are trained to optimize performance on the E2EST task. Experimental results on the MuST-C speech translation benchmark demonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on En-De/En-Fr/En-Es language pairs, surpassing previous models and establishing a new state-of-the-art. Additionally, we conduct an in-depth analysis of single-modal model selection and the impact of training strategies, which lays the foundation for future research. We will open up our code and models after review.

  Translated Abstract: 随着大语言模型（LLM）的出现，基于LLM的多模式模型具有巨大的潜力。诸如LLASM，X-LLM和Speekgpt之类的模型具有理解和产生人类指示的令人印象深刻的能力。但是，当面对复杂的任务（例如端到端语音翻译（E2E-ST））时，他们的性能通常会摇摆不定，这是一项跨语言和跨模式翻译任务。与单模模型相比，在这些情况下，多模型落后于落后。本文介绍了LST，这是一种旨在在E2E-ST任务中表现出色的大型多模式模型。LST由演讲前端，适配器和LLM后端组成。LST的培训由两个阶段组成：（1）模态调整，调整适配器以使语音表示与文本嵌入空间保持一致，以及（2）下游任务微调，适配器和LLM模型均经过训练以优化E2ST任务的性能。关于必不可少的语音翻译基准的实验结果表明，LST-13B在EN-DE/EN-DE/EN-FR/EN-ES语言对上的BLEU得分达到30.39/41.55/35.33艺术。此外，我们对单模模型选择和培训策略的影响进行了深入的分析，这为未来的研究奠定了基础。审查后，我们将打开代码和模型。


54. Title: Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward  Reasoning in Math Word Problems
   Link: https://arxiv.org/abs/2310.01991
   Classification Answer: yes

   Abstract: While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information?
In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought.

  Translated Abstract: 尽管在最近的文献中已经广泛探索了远期推理（即找到给定问题的答案），但向后推理相对尚未探索。我们检查了LLM在数学单词问题（MWPS）上的向后推理功能：给定数学问题及其答案，并从该问题中省略了一些细节，LLM可以有效地检索缺失的信息吗？
在本文中，我们正式定义了数学单词问题上的向后推理任务，并修改三个数据集以评估此任务：GSM8K，SVAMP和Multiarith。与四个SOTA LLM（GPT4，GPT3.5，Palm-2和Llama-2）的前进推理相比，我们的发现表明，模型在向后推理的准确性显着下降。利用此任务的特定格式，我们提出了三种提高性能的新颖技术：重新培训将给定的问题重新定义为前向推理问题，Pal-Tool结合了程序辅助LLM的想法，以产生一组可以通过的方程式来解决的方程式外部求解器，并检查您的工作可利用向前方向上高精度的自然验证者的可用性，从而交错了解决和验证步骤。最后，意识到我们的每种基本方法都正确地解决了一组不同的问题，我们提出了一种新颖的贝叶斯公式，用于在这些基本方法上创建合奏，这是通过验证者帮助的，以进一步提高准确性，从而通过显着的边距提高准确性。广泛的实验表明，我们的技术依次提高了LLM在向后推理任务上的性能，最终基于合奏的方法可与RAW LLM相比具有带有标准提示技术（例如Thearks of Thought）的RAW LLM的实质性增长。


55. Title: Ring Attention with Blockwise Transformers for Near-Infinite Context
   Link: https://arxiv.org/abs/2310.01889
   Classification Answer: yes

